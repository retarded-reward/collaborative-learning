\documentclass[compress]{beamer}
\usepackage[
    title={Collaborative Learning},
    subtitle={Una soluzione di reinforcement learning applicata a un nodo router di messaggi},
    event={Progetto fine corso Machine Learning},
    author={DLP, MC, LF},
    longauthor={Daniele La Prova, Matteo Conti, Luca Falasca},
    email={},
    institute={ML 2023-2024},
    longinstitute={Universita' degli Studi di Roma Tor Vergata},
]{unislides}
\usepackage{graphicx} % Required for inserting images
\usepackage{minted}
\usepackage{algorithm}
\usepackage{hyperref}
\usepackage{adjustbox}
\usepackage{svg}
\svgsetup{inkscapelatex=false}

\begin{document}

\begin{frame}[plain]
    \titlepage
\end{frame}

\section{Introduzione}
%TODO:

\subsection{Contesto}
\begin{frame}{Contesto}
    In un sistema moderno i QoS possono essere di vario genere e non sempre sono legati alle sole prestazioni, in particolare, hanno acquisito sempre più rilevanza aspetti come la sostenibilità ed il valore portato dal sistema.\\ E' quindi necessario trovare delle soluzioni che siano flessibili rispetto a tali aspetti in modo da poter essere adattate a diversi contesti.
    \vspace{2cm}
    \begin{adjustbox}{margin=0.4cm 0cm 0cm 0.65cm, center} % left, bottom, right, top
        \includegraphics[width=.6\textwidth]{figs/qos_adapt.png}
    \end{adjustbox}
\end{frame}
\begin{frame}{\subsecname}
    Il progetto verte sulla realizzazione di un nodo di rete dotato di code di priorità e diverse fonti energetiche il cui comportamento è determinato da un agente di reinforcement learning che cerca di minimizzare una reward basata sui seguenti QoS:
    \begin{columns}
        \column{0.5\textwidth}
            \begin{minipage}[b]{1\textwidth}
                \begin{itemize}
                    \item Costo energetico
                    \item Occupazione delle code
                    \item Perdita di pacchetti dalle code 
                \end{itemize}
            \end{minipage}
            \column{0.5\textwidth}
                \begin{minipage}{1\textwidth}
                    \begin{adjustbox}{margin=0cm 0cm 0cm 0cm, center} % left, bottom, right, top
                        \includegraphics[width=.65\textwidth]{figs/agent_icon.png}
                    \end{adjustbox}
                \end{minipage}
    \end{columns}
\end{frame}
\subsection{Obiettivi}

%TODO:

\section{Metodologia}
%TODO:

\subsection{Agente}
%TODO:

\subsubsection{AgentFaçade}
%TODO:

\subsubsection{AgentFactory}
%TODO:

\subsubsection{Decisions}
%TODO:

\subsection{Simulazione}
\begin{frame}{Simulazione}
    Per la simulazione è stato utilizzato il framework OMNeT++, il quale permette di definire e simulare una rete di nodi. \\Dato che OMNeT++ non supporta nativamente Python per permettere la comunicazione tra l'agente ed il nodo, è stata utilizzata la libreria PyBind la quale permette di esporre simboli in Python e C++ e farne il binding.
    \begin{adjustbox}{margin=0cm 0cm 0cm 0.5cm, center} % left, bottom, right, top
        \includegraphics[width=.8\textwidth]{figs/pybind_omnet.png}
    \end{adjustbox}
\end{frame}

\subsubsection{Nodo}
\begin{frame}{Nodo}
    E' l'elemento della rete che è in grado di ricevere e inviare messaggi sulla base della scelta dell'agente, è composto da:
    \begin{columns}
        \column{0.5\textwidth}
            \begin{minipage}[b]{1\textwidth}
                \begin{itemize}
                    \item Controller, che implementa la logica del nodo
                    \item AgentClient, che permette la comunicazione con l'agente
                    \item Queues, una o più code che accodano messaggi
                \end{itemize}
            \end{minipage}
        \column{0.5\textwidth}
            \begin{minipage}{1\textwidth}
                \begin{adjustbox}{margin=0cm 0cm 0.1cm 0.2cm, center} % left, bottom, right, top
                    \includegraphics[width=1\textwidth]{figs/node_layout_3queues.png}
                \end{adjustbox}
            \end{minipage}
    \end{columns}
\end{frame}

\subsubsection{Nodo - Controller}
\begin{frame}{Nodo - Controller (1)}
E' il componente che implementa la logica del nodo, in particolare:
\vspace{0.5cm}
    \begin{columns}
        \column{0.6\textwidth}
            \begin{minipage}[b]{1\textwidth}
                \begin{itemize}
                    \item Campiona lo stato visibile al nodo
                    \item Interroga l'agente per ottenere l'azione consegnandogli la reward dell'azione precedente
                    \item Attua l'azione ricevuta dall'agente
                    \item Calcola la reward per le azioni ricevute dall'agente
                \end{itemize}
            \end{minipage}
        \column{0.4\textwidth}
            \begin{minipage}{.9\textwidth}
                \begin{adjustbox}{margin=0.5cm 0cm 0.1cm 0.2cm, center} % left, bottom, right, top
                    \includegraphics[width=1\textwidth]{figs/control_loop.png}
                \end{adjustbox}
            \end{minipage}
    \end{columns}
\end{frame}

\begin{frame}{Nodo - Controller (2)}
    Il controller si occupa anche di gestire le sorgenti energetiche del nodo, in particolare:
    \vspace{0.5cm}
        \begin{columns}
            \column{0.6\textwidth}
                \begin{minipage}[b]{1\textwidth}
                    \begin{itemize}
                        \item Una batteria, la cui carica dipende da fattori ambientali
                        \item Una presa di corrente, che ha un costo di utilizzo
                    \end{itemize}
                \end{minipage}
            \column{0.4\textwidth}
                \begin{minipage}{.9\textwidth}
                    \begin{adjustbox}{margin=0.5cm 0cm 0.1cm 0.2cm, center} % left, bottom, right, top
                        \includegraphics[width=1\textwidth]{figs/socket_battery.png}
                    \end{adjustbox}
                \end{minipage}
        \end{columns}
    \end{frame}

\subsubsection{Nodo - Agent Client}
\begin{frame}{Nodo - Agent Client}
    E' il componente che fa da intermediario tra il controller e l'agente, in particolare:
    \begin{itemize}
        \item Accoglie le action request del controller, le quali contengono stato e reward dell'azione precedente 
        \item Converte l'action request in una bean compatibile con l'implementazione Python dell'agente
        \item Inoltra la bean all'agente
        \item Accoglie le bean di risposta dell'agente, le quali contengono l'azione da attuare
        \item Converte la bena di risposta in una action response compatibile con l'implementazione in C++ del controller
        \item Inoltra l'action response al controller
    \end{itemize}
\end{frame}

\subsubsection{Nodo - Queue}
\begin{frame}{Nodo - Queue}
    E' il componente che implementa la logica della coda, in particolare:
    \vspace{0.5cm}
        \begin{columns}
            \column{0.6\textwidth}
                \begin{minipage}[b]{1\textwidth}
                    \begin{itemize}
                        \item Accoglie i pacchetti in arrivo, scartandoli se la coda è piena
                        \item Riceve le QueueDataRequest di pacchetti da parte del controller
                        \item Consegna i pacchetti al controller in delle QueueDataResponse
                        \item Comunica al controller gli aggiornamenti sullo stato della coda, tramite una QueueStateUpdate, ogni volta che arriva un pacchetto
                    \end{itemize}
                \end{minipage}
            \column{0.4\textwidth}
                \begin{minipage}{.9\textwidth}
                    \begin{adjustbox}{margin=0.5cm 0cm 0.1cm 0cm, center} % left, bottom, right, top
                        \includegraphics[width=1\textwidth]{figs/queue_scheme.png}
                    \end{adjustbox}
                \end{minipage}
        \end{columns}
\end{frame}

\subsubsection{SrcNode}
\begin{frame}{SrcNode}
    E' l'elemento della rete che si occupa di generare il traffico, ogni Queue ha associato un SrcNode.
    Il tempo di interarrivo dei pacchetti è regolato da una distribuzione esponenziale con tasso $\lambda$ configurabile, mentre la dimensione dei pacchetti è regolata da una distribuzione uniforme su un intervallo configurabile.
    \begin{adjustbox}{margin=0.5cm 0cm 0.5cm 0.5cm, center} % left, bottom, right, top
        \includegraphics[width=.8\textwidth]{figs/src_scheme.png}
    \end{adjustbox}
\end{frame}

\subsubsection{Network}
\begin{frame}{Network}
E' l'ambiente in cui vivono tutti gli elementi della simulazione, qui vengono specificati i collegamenti tra i SrcNode e le code del nodo router. 
    \begin{adjustbox}{margin=0cm 0cm 0cm 0cm, center} % left, bottom, right, top
        \includegraphics[width=.65\textwidth]{figs/network_layout_3queues.png}
    \end{adjustbox}
\end{frame}


\section{Risultati}
\begin{frame}
    \frametitle{\secname}
    \begin{itemize}
        \item Sono stati condotti diversi esperimenti per valutare le prestazioni del nodo in base 
        alle azioni suggerite dal suo agente
        \item Per effettuare gli esperimenti si è partiti da una configurazione dei parametri di 
        base\footnote{\href{https://github.com/retarded-reward/collaborative-learning/blob/main/simulations/res/omnetpp.ini}{https://github.com/retarded-reward/collaborative-learning/blob/main/simulations/res/omnetpp.ini}},
        a cui poi sono state apportate modifiche a seconda dello scenario considerato
        nell'esperimento.
        \item Negli esperimenti andiamo a confrontare tre tipologie di agenti:
        \begin{itemize}
            \item DQN: Agente DQN implementato secondo le specifiche delle Decisions
            \item DQN flat: Agente DQN in cui le azioni sono l'enumerazione di tutte
             le possibili azioni (Albero delle decisions con 1 singolo nodo). 
            \item Random: Agente che sceglie casualmente tra le azioni,
             utilizzato per avere un benchmark di confronto.
        \end{itemize}
    \end{itemize}
\end{frame}

\subsection{Coda singola, reward bilanciata}

\begin{frame}
    \frametitle{\secname: \subsecname}
    \begin{figure}
        \centering
        \includesvg[scale = 0.19]{figs/results_charts/cumulativeReward_default_allAgents.svg}
        \label{fig:cumulative_reward_dqnDeep_dqnFlat_random_defaultConfig}
    \end{figure}
    \begin{itemize}
        \item l'agente DQN sembra imparare una politica più vantaggiosa dopo
         50 secondi di simulazione
        \item Il DQN con decisions sembra
        imparare più lentamente rispetto al DQN flat, per poi peggiorare
    \end{itemize}
\end{frame}
\begin{frame}
    \frametitle{\secname: \subsecname}
    \begin{figure}
        \centering
        \includesvg[scale = 0.19]{figs/results_charts/sendCount_allAgents_defaultConfig.svg}
        \label{fig:sendCount_allAgents_defaultConfig}
    \end{figure}
    \begin{itemize}
        \item l'agente root sembra essere scoraggiato dall'inviare pacchetti a causa delle scelte di agenti
        più in profondità nell'albero delle decisions, ad esempio se l'agente responsabile della
        scelta della power source sceglie spesso di ricorrere al power chord.
    \end{itemize}
\end{frame}
\begin{frame}
    \frametitle{\secname: \subsecname}
    \begin{figure}
        \centering
        \includesvg[scale = 0.19]{figs/results_charts/energySaving_defaultConfig_allAgents.svg}
        \label{fig:energySaving_defaultConfig_allAgents}
        \end{figure}
    \only<+> {\begin{itemize}
        \item gli agenti DQN e DQN flat presentano un risparmio di spesa energetica più basso (6\%, 8\%) rispetto al Random (14\%)
        \item gli agenti DQN sembrano inviare più spesso pacchetti dalla power chord piuttosto che dalla batteria
    \end{itemize}}
    \only<+>{ \begin{itemize}
        \item  Ciò potrebbe essere dovuto al fallback dato che nello stato dell'agente non è presente alcuna informazione relativa al consumo energetico
    \end{itemize}}
\end{frame}
\begin{frame}
    \frametitle{\secname: \subsecname}
    \begin{figure}
        \centering
        \includesvg[scale = 0.19]{figs/results_charts/occupancies_allAgents_defaultConfig.svg}
        \label{fig:occupancies_allAgents_defaultConfig}
    \end{figure}
    \begin{itemize}
        \item gli agenti
        DQN e DQN flat riescano a gestire meglio la popolazione della coda rispetto al
        Random
        \item in certi periodi
        si concentrano nell'invio di pacchetti continuo, mentre in altri non fanno niente
    \end{itemize}    
\end{frame}
\begin{frame}
    \frametitle{\secname: \subsecname}
    \begin{figure}
        \centering
        \includegraphics[scale = 0.24]{figs/results_charts/actionOverTime_allAgents_defaultConfig_annotated.png}
        \label{fig:actionOverTime_allAgents_defaultConfig_annotated}
        \end{figure}
    \begin{itemize}
        \item  il DQN con decision oscilla di meno rispetto alla versione flat, anche se
        accumula più reward negativa.
    \end{itemize}    
\end{frame}

\subsection{Coda singola, full power saving}
\begin{frame}
    \frametitle{\secname: \subsecname}
    \begin{figure}
        \includesvg[scale=0.19]{figs/results_charts/allPowerSaving_allAgents_defaultConfig.svg}
        \label{fig:allPowerSaving_allAgents_defaultConfig}
    \end{figure}    
    \begin{itemize}
        \item il DQN con decisions sembra 
        andare meglio rispetto alla controparte flat
        \item la decision root deve scegliere solo tra due azioni e quindi
        impara più in fretta che non conviene inviare mai rispetto al DQN flat
    \end{itemize}
\end{frame}

\subsection{Coda singola, no power saving}
\begin{frame}
    \frametitle{\secname: \subsecname}
    \begin{figure}
        \centering
        \includesvg[scale=0.19]{figs/results_charts/noPowerSaving_allAgents_defaultConfig.svg}
        \label{fig:noPowerSaving_allAgents_defaultConfig}
    \end{figure}
        \begin{itemize}
        \item le varianti di DQN accumulino un quantitivo di reward simile rispetto allo
        scenario con reward bilanciata, mentre il random va peggio.
        \item a pesare
        maggiormente sulla reward sono i termini riguardanti le code, e la
        strategia imparata dal DQN punta sempre a minimizzarli.
    \end{itemize}
\end{frame}
\subsection{Coda multipla, reward bilanciata}
\begin{frame}
    \frametitle{\secname: \subsecname}
    \begin{figure}
        \includesvg[scale=0.19]{figs/results_charts/balanced_allAgents_5queues_300s.svg}
        \label{fig:balanced_allAgents_5queues_300s}
    \end{figure}
        \begin{itemize}
        \item gli agenti 
        DQN hanno un comportamento peggiore del random, segno che non sono riusciti a imparare una 
        strategia appropriata
        \item il DQN con decisions sembra soffrire di meno rispetto
        alla variante flat.
    \end{itemize}
\end{frame}

\section{Conclusioni}
\begin{frame}
    \frametitle{\secname}
    \begin{itemize}
        \onslide<+-> \item All'aumentare del numero delle code la difficoltà di apprendimento dell'agente
        aumenta considerevolmente, per cui forse è opportuno considerare un modello diverso;
        \onslide<+-> \item L'utilità dell'uso di un albero delle decisioni sembra essere limitata
        rispetto a un DQN normale, anche se sembra migliorare le prestazioni del DQN
        con'aumentare del numero delle azioni possibili;    
    \end{itemize}
\end{frame}
\begin{frame}
    \frametitle{\secname}
    \begin{itemize}
        \onslide<+-> \item Nello sviluppo di soluzioni di reinforcement learning è di importanza fondamentale
        che il modello di stato, azioni e reward utilizzati siano rappresentativi del 
        problema, validi e verificati, poichè un minimo errore ad esempio nel calcolo della
        reward può indurre comportamenti nell'agente molto differenti e difficilmente
        spiegabili;
        \onslide<+-> \item Apportare delle soluzioni che permettono di non dimenticare facilmente
        esperienze di exploration può aiutare l'agente a imparare una strategia ottima;
    \end{itemize}        
\end{frame}
\begin{frame}
    \frametitle{Grazie per l'attenzione!}
    \begin{itemize}
        \item Tutto il codice che implementa ambiente di simulazione e agente è disponibile al 
        seguente repository: \href{https://github.com/retarded-reward/collaborative-learning}{https://github.com/retarded-reward/collaborative-learning}
        \item Consultare README per dettagli build, e la relazione per ulteriori approfondimenti;
        \item contattaci a:
            \begin{itemize}
                \item \href{mailto:daniele.laprova@students.uniroma2.eu}{daniele.laprova@students.uniroma2.eu}
                \item \href{mailto:matteo.conti@students.uniroma2.eu}{matteo.conti@students.uniroma2.eu}
                \item \href{mailto:luca.falasca@students.uniroma2.eu}{luca.falasca@students.uniroma2.eu}
            \end{itemize}
        \item Domande ???
        \item Fatto con \includegraphics[scale=0.5]{figs/icons/icons8-heart-24.png} \includegraphics[scale=0.4]{figs/icons/icons8-coffee-cup-48.png}
    \end{itemize}
\end{frame}


\end{document}
