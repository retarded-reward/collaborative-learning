\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{dblfloatfix}
\usepackage[colorlinks=true, linkcolor=blue, urlcolor=blue, citecolor=green]{hyperref}
\usepackage{minted}
\usepackage{microtype}
%\usepackage{parskip}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\let\subsectionautorefname\sectionautorefname
\let\subsubsectionautorefname\sectionautorefname
\title{Collaborative Learning\\ \large Una soluzione di reinforcement learning\\ applicata a un nodo router di messaggi\\
{\footnotesize Progetto Machine Learning}
}
\author{\IEEEauthorblockN{Matteo Conti}
\IEEEauthorblockA{\textit{0323728}} \\
\and
\IEEEauthorblockN{Daniele La Prova}
\IEEEauthorblockA{\textit{0320429}} \\
\and
\IEEEauthorblockN{Luca Falasca}
\IEEEauthorblockA{\textit{0334722}} \\
}

\newcommand{\code}[1]{\texttt{#1}}

\maketitle
\thispagestyle{plain}
\pagestyle{plain}

\begin{abstract}
%TODO:
\end{abstract}

\section{Introduzione}
\subsection{Contesto}
In un sistema moderno i QoS possono essere di vario genere e non sempre sono legati alle sole prestazioni, in particolare, hanno acquistato sempre più rilevanza aspetti come il valore portato dal sistema e la sostenibilità dello stesso. In un sistema di comunicazione, ad esempio, il valore portato da un messaggio può variare a seconda del tempo che impiega ad essere processato, allo stesso tempo un messaggio inviato utilizzando energia green può avere un valore maggiore rispetto a un messaggio inviato tramite energia tradizionale.

Nel nostro progetto viene fatto il design di un nodo di rete dotato di un agente di reinforcement learning che impara a prendere decisioni sulla base di un modello di reward che tiene conto di diversi QoS come il consumo energetico, l'occupazione delle code ed il numero di pacchetti persi. L'agente è in grado di interagire con l'ambiente e di apprendere da queste interazioni, in modo da ottimizzare il suo comportamento e massimizzare la reward accumulata. Il modello di reward è configurabile in modo da dare più o meno peso a ciascuna delle metriche di QoS, in modo da poter adattare il comportamento dell'agente a diversi scenari.

\subsection{Obiettivi}
%TODO:

\section{Metodologia}
%TODO:

\subsection{Modello}
A partire dalla specifica del problema è stato definito un modello che delinea come
sono stati rappresentati lo Stato, le Azioni e le Reward della simulazione.

\subsubsection{Stato}
Per catturare lo stato della simulazione è stato deciso di campionare a ogni timestep le
seguenti variabili:
\begin{itemize}
    \item Percentuale di carica della batteria;
    \item Quanta percentuale di batteria è stata ricaricata nell'ultimo timestep;
    \item La percentuale di occupazione delle code, un valore per ogni coda. 
\end{itemize}
Tali valori sono stati discretizzati ove possibile per permetterne la comprensione da
parte di alcune implementazioni di agenti, ad esempio il DQN.
\subsubsection{Azioni}
Un nodo può essenzialmente svolgere due operazioni in un qualunque timestep:
\begin{itemize}
    \item Non fare nulla
    \item Inviare un pacchetto
\end{itemize}
Qualora l'azione scelta sia quella di inviare un pacchetto, allora è necessario anche
scegliere:
\begin{itemize}
    \item Da quale coda prelevare il pacchetto
    \item Da quale power source attingere l'energia necessaria per l'invio
\end{itemize}
\subsubsection{Reward}
La reward $r$ da inviare all'agente per un qualunque timestep $t$ è calcolata secondo la
seguente formula:
\begin{equation}
    \label{eq:reward}
    \begin{aligned}
        & r(t) = \sum_{i \in T} w_i \cdot r_i(t), & -1 \leq r_i \leq 0\\
        && 0 \leq w_i \leq 1, \\
        && \sum_{i \in T} w_i = 1, \\
        && T = \{e, o, d\},\\
        && t \geq 0 \in \mathbb{N}
    \end{aligned}
\end{equation}
In sostanza, la reward è calcolata come la somma pesata dei contributi normalizzati di
diversi termini.
I pedici $\{e, o, d\}$ si riferiscono rispettivamente ai contributi derivati dal termine
energetico, di occupazione delle code e di drop dei pacchetti in quel timestep.

Il termine energetico è calcolato come:
\begin{equation}
    \label{eq:reward_energy}
    \begin{aligned}
        & r_e(t) = -\sum_{p \in P}\frac{\text{cons}_p(t) \cdot \text{cost}_p}{\max_{\tau < t}\{\text{cons}_p(\tau)\} \cdot \sum_{p \in P}\text{cost}_p},\\
        & P = \{\text{battery}, \text{power chord}\} 
    \end{aligned}
\end{equation}
Al numeratore abbiamo il prodotto tra il consumo energetico della power source $p$ e il
suo costo di utilizzo, normalizzato sul massimo consumo energetico registrato fino
a quel time step per quella power source moltiplicato per la somma dei costi di utilizzo.
La batteria
ha un costo di utilizzo pari a zero ma una capacità limitata e una ricarica influenzata
da fattori casuali, mentre il Power Chord ha un costo di utilizzo diverso da zero ma
una capacità pressoché illimitata.

Il termine dell'occupazione delle code è calcolato come:
\begin{equation}
    \label{eq:reward_occupancy}
    \begin{aligned}
        & r_o(t) = -\sum_{q \in Q}\frac{\text{occ}_q(t) \cdot p_q}{\max_{\tau < t}\{\text{occ}_q(\tau)\} \cdot \sum_{q \in Q}p_q},\\
        & Q = \{0, 1, \dots \#Q - 1\} 
    \end{aligned}
\end{equation}
Al numeratore abbiamo il prodotto tra la percentuale di occupazione della coda $q$ nel 
timestep $t$ e la sua priorità $p_q$, normalizzato sul massimo valore di occupazione
registrato fino a quel time step per quella coda moltiplicato per la somma delle priorità. 
Essendo il termine dell'occupazione delle code una percentuale e la priorità definita
come $p_q = q + 1$ possiamo riscrivere \autoref{eq:reward_occupancy} come:
\begin{equation}
    \label{eq:reward_occupancy_simplified}
    \begin{aligned}
        & r_o(t) = -\sum_{q \in Q}\frac{\text{occ}_q(t) \cdot (q + 1)}{100 \cdot \frac{1}{2}\#Q(\#Q + 1)},\\
        & Q = \{0, 1, \dots \#Q - 1\} 
    \end{aligned}
\end{equation}

Il termine del drop dei pacchetti delle code è calcolato come:
\begin{equation}
    \label{eq:reward_drop}
    \begin{aligned}
        & r_d(t) = -\sum_{q \in Q}\frac{\text{drop}_q(t) \cdot p_q}{\text{inbound}_q(t) \cdot \sum_{q \in Q}p_q},\\
        & Q = \{0, 1, \dots \#Q - 1\} 
    \end{aligned}
\end{equation}
Al numeratore abbiamo il prodotto tra il numero di pacchetti persi dalla coda $q$ nel 
timestep $t$ e la sua priorità $p_q$, normalizzato sul numero di pacchetti arrivati alla coda in quel time step moltiplicato per la somma delle priorità.

In un modo analogo per scrivere \autoref{eq:reward_occupancy_simplified} possiamo
riscrivere \autoref{eq:reward_drop} come:
\begin{equation}
    \begin{aligned}
        & r_d(t) = -\sum_{q \in Q}\frac{\text{drop}_q(t) \cdot (q + 1)}{\text{inbound}_q(t) \cdot \frac{1}{2}\#Q(\#Q + 1)},\\
        & Q = \{0, 1, \dots \#Q - 1\} 
    \end{aligned}
\end{equation}

Con i termini della reward così definiti, è possibile dimostrare che $ -1 \leq r_i \leq 0 \, \forall i \in T$, e dunque tutti i termini della reward normalizzati sono 
confrontabili.

\subsection{Agente}
%TODO:

\subsubsection{AgentFaçade}
L'Agent Façade rappresenta un'interfaccia che espone i metodi necessari per l'interazione con l'agente. Questa soluzione consente di nascondere tutte le interazioni tra i sottosistemi e l'agente, offrendo un'interfaccia unificata per la comunicazione con l'agente stesso. Nella nostra implementazione, non è l'agente a restituire direttamente l'azione da intraprendere; piuttosto, è il simulatore (o, in uno scenario reale, il nodo) che richiede, dato un determinato stato, quale azione intraprendere in quel momento. Ciò implica che il simulatore scandisce il tempo. In questo modo, l'agente non è consapevole del tempo trascorso e può facilmente adattarsi a uno scenario reale.
\subsubsection{AgentFaçade - Configurazione} 
Per utilizzare l'AgentFaçade è necessario configurare l'agente. La configurazione può essere fatta tramite un file in cui è possibile specificare varie caratteristiche dell'agente in modo da poterlo adattare a diversi scenari. La documentazione per la configurazione è disponibile qui\footnote{\href{https://github.com/retarded-reward/collaborative-learning/wiki/Agent-Configuration}{https://github.com/retarded-reward/collaborative-learning/wiki/Agent-Configuration}}. Un'altro parametro di configurazione da specificare all'AgentFaçade è il numero di code presenti nel nodo. Questo parametro è necessario per l'agente in modo da poter generare azioni valide.
\subsubsection{AgentFaçade - Rappresentazione}
Per rappresentare lo stato è stato fatto uso di un Tensore unidimensionale, in cui
ogni componente rappresenta una variabile dello stato. Questo vuol dire che variabili
di stato multidimensionali, come ad esempio le occupancies delle code, sono state
appiattite in un vettore unidimensionale, e poi concatenate alle altre variabili in modo
da produrre un unico Tensore.

L'azione che il nodo può intraprendere è composta da valori di diverse componenti,
ognuna delle quali è rappresentata da un valore scalare intero. Come è spiegato meglio
nella sezione delle \autoref{subsec:Decisions}, ogni componente dell'azione è chiamata Decision, e 
la combinazione di Decisions di diversi agenti produce l'azione da intraprendere.
\subsubsection{AgentFaçade - Interazione}
Il metodo principale per interagire con l'Agent Façade è \code{get\_action}, che restituisce l'azione da intraprendere in base allo stato attuale. Questo metodo prende in input lo stato attuale e la reward associata all'azione precedente, e restituisce l'azione da intraprendere. All'interno di questo metodo, l'Agent Façade addestra l'agente con l'esperienza passata e interroga l'agente per ottenere l'azione da intraprendere utilizzando l'albero delle Decisions  (vedi \autoref{subsec:Decisions}). L'addestramento non avviene ad ogni chiamata, ma secondo un intervallo di step configurabile. Questo intervallo è necessario per evitare che l'agente si adatti troppo velocemente a uno scenario specifico e non sia in grado di generalizzare, oltre a migliorare le prestazioni computazionali.

\subsubsection{AgentFactory}
L'Agent Factory è una classe che configura e crea diversi agenti disponibili nella libreria. Nel nostro caso, abbiamo utilizzato il DQN (Deep Q-Network) ed un RandomAgent come benchmark. Inoltre, è stato implementato un decoratore per gli agenti per integrarli dinamicamente con i replay buffer forniti dalla libreria. Utilizzando il metodo \code{create\_agent}, è possibile creare un'istanza di un agente specificando il tipo di agente e i parametri necessari per la configurazione. Questa operazione viene eseguita all'interno dell'AgentFaçade.

\subsubsection{Decisions}
\label{subsec:Decisions}
Lo spazio di decisioni in cui un agente può trovarsi ad operare può estendersi
attraverso
diverse dimensioni, ognuna delle quali rappresenta un parametro che costituisce
tale azione. Ad esempio, nel caso del problema in esame un'azione dell'agente è
rappresentabile da un vettore a tre dimensioni così codificato:
\begin{itemize}
\item Azione[0] = invia o non fare nulla;
\item Azione[1] = preleva pacchetto da coda i, con $i \in [0, \code{num\_queues} - 1]$;
\item Azione[2] = seleziona power source j, $j \in [0, \code{num\_power\_sources} - 1]$.
\end{itemize}
Si noti che non tutte le combinazioni di valori di questi parametri
rappresentino dei punti legali all'interno dello spazio delle azioni.
Ad esempio, se la prima componente è pari a 0 (non fare nulla), allora i valori delle
altre componenti devono essere don't care.

Una soluzione può essere ricorrere a un'enumerazione, ovvero si "schiaccia"
lo spazio delle azioni lungo un'unica dimensione i cui valori sono rappresentati 
dalle sole combinazioni che rappresentano azioni legali. È una soluzione semplice e
immediata, tuttavia comporta alcuni problemi:
\begin{itemize}
    \item la dimensione dello spazio delle azioni aumenta molto velocemente rispetto
    all'aumentare delle opzioni disponibili. Nel caso del problema, avere in generale
    10 code e 2 power sources comporta 20 possibili azioni diverse. Aggiungere una
    sola power source aggiunge altre 10 azioni disponibili all'agente.
    \item come conseguenza del primo problema, l'apprendimento dell'agente può
    rallentare. %TODO: inserire calcolo reward
\end{itemize}
Le decisions sono una possibile soluzione che combina i vantaggi dell'enumerazione
cercando di limitarne gli svantaggi. Il concetto chiave consiste nel delegare la scelta
del valore di ogni componente di un'azione a un agente diverso. In questa maniera,
ogni agente è responsabile di prendere una decisione, e l'insieme di decisioni compone
l'azione da intraprendere. 

Talvolta può accadere che alcune decisioni vadano intraprese solo se le decisioni prese
precedentemente lo consentono. Ad esempio, se una decisione ha dato come esito
"non fare nulla", non ha senso che si interroghino gli agenti responsabili
della decisione su quale coda e quale power source usare. Per coprire questo aspetto,
gli agenti che prendono le decisioni sono organizzati in un albero, detto appunto
albero delle decisioni. Il nodo root contiene un riferimento all'agente che prende
la decisione iniziale, che dovrebbe essere di alto livello.
A questo punto possono verificarsi due casi:
\begin{itemize}
    \item Il nodo root è una foglia: Nell'albero esiste solo il nodo root. questo
    è il caso banale in cui l'albero di decisione regredisce a un singolo agente.
    La decisione corrisponde al valore ritornato dall'agente, che dunque è anche
    l'azione da intraprendere.
    \item Il nodo root ha figli: La root dispone di più scelte per la decisione.
    Ad ogni scelta corrisponde un nodo figlio, a cui verrà delegata la successiva
    decisione. Il nodo root interroga l'agente associato alla scelta selezionata
    dal suo agente, e così via fino a raggiungere una foglia.
\end{itemize}
La sequenza di decisioni
intraprese dagli agenti rappresenta un cammino dalla root a una foglia dell'albero
delle decisioni, ed è chiamato decision path.
Ogni nodo interrogato registra la sua decisione all'interno del decision path, 
associando alla propria posizione nel path il nome della sua decisione e il suo
valore. Un esempio è visibile in \autoref{fig:decisions}.

Ogni nodo dell'albero delle decisioni è un Consultant. Un Consultant si occupa
delle seguenti responsabilità:
\begin{itemize}
    \item mantiene un riferimento all'agente che prende la decisione;
    \item Mantiene una lista di consultant figli, ognuno dei quali corrisponde
    a una possibile scelta che il suo agente può prendere;
    \item espone un metodo \code{get\_decisions()}(\autoref{fig:decisions_getDecisions_activity_diagram}), che prende in input
    lo stato e un decision path (possibilmente vuoto) e aggiunge il suo contributo
    al decision path interrogando il suo agente;
    \item espone un metodo \code{train()}(\autoref{fig:decisions_train_activity_diagram}), che prende in input un'Esperienza,
    ovvero una tripla
    decision path, stato, reward e addestra il suo agente e quello dei nodi
    figli che fanno parte del decision path.
\end{itemize}
Lo stato e l'esperienza presi in input da un Consultant sono a loro volta
propagati ai consultant figli che fanno parte del decision path. Inoltre,
ogni Consultant può raffinare lo stato e l'esperienza presi in input dal padre
se vengono specificate delle implementazioni per i metodi \code{deduce\_consultant\_state()}
e \code{deduce\_consultant\_experience()}. Tuttavia, tali deduzioni non sono propagate ai figli
per impedire perdite di informazioni. 

Le caratteristiche delle decisions descritte finora comportano
le seguenti implicazioni:
\begin{itemize}
    \item Tutti gli agenti dei Consultant di uno stesso decision path condividono
    lo stesso stato osservato dalla root, oppure ne osservano una deduzione.
    Per esempio, un consultant potrebbe scartare alcuni parametri dello stato
    poiché non rilevanti per la decisione che deve prendere;
    \item Tutti gli agenti dei Consultant di uno stesso decision path condividono
    la stessa esperienza della root al momento del training, oppure ne osservano
    una deduzione. Ad esempio, un consultant potrebbe
    usare come reward un valore derivato da quello osservato dalla root;
    \item Agenti che non fanno parte di un decision path non sono interrogati al momento
    della richiesta delle decisioni per quel path e non partecipano al training corrispondente.
    Questo vuol dire che tali agenti osserveranno solo gli stati che implicano una loro
    partecipazione nel decision path e rewards che sono frutto di essa;
    \item L'addestramento dei consultant di un decision path può essere fatto in parallelo;
    \item Consultant in profondità nell'albero delle decisioni potrebbero ricevere molte meno
    esperienze, a seconda che i consultant dei livelli superiori abbiano preso decisioni
    che portino a un loro coinvolgimento oppure no. Sebbene ciò possa sembrare che ne rallenti
    l'apprendimento, bisogna considerare che tali consultant dispongono di uno spazio delle azioni
    molto più ridotto rispetto a quello di un singolo, monolitico agente che deve orientarsi
    in uno spazio delle azioni che enumera tutte le possibili combinazioni legali di valori
    delle componenti delle azioni. Inoltre, se non ricevono esperienze vuol dire che
    non vengono coinvolti così spesso nella costruzione di un decision path. In sostanza,
    un consultant in profondità nell'albero delle decisioni riceve meno esperienze, ma
    ne necessita di meno affinché l'apprendimento converga;
    \item Agenti di consultants in cima all'albero delle decisions potrebbero essere
    portati a ritenere alcuni dei propri rami come sconvenienti a causa di scelte
    sfortunate dei consultant più in profondità, e potrebbero avere difficoltà
     a scoprire che
    effettuare la stessa scelta potrebbe portare a un esito migliore in futuro se
    i consultant in profondità scelgono differentemente.
\end{itemize}

In sostanza, l'obbiettivo del framework delle decisions è quello di poter costruire un albero
delle decisioni dalla cui collaborazione dei consultant emerga un comportamento
equivalente a quello di un agente monolitico basato su un'enumerazione dello spazio
delle azioni, senza dover esserne vincolati dagli svantaggi. Da questa idea
viene il nome di collaborative learning.

\subsection{Simulazione}
Per la simulazione è stato utilizzato il framework OMNeT++\footnote{\href{https://omnetpp.org/}{https://omnetpp.org/}}, il quale offre degli strumenti per definire e simulare in modo semplice e modulare una rete di nodi. In particolare, il framework è stato utilizzato per simulare un nodo router che riceve traffico da più nodi sorgente e prende decisioni sulle basi dell'agente di reinforcement learning montato sul nodo. Dato che OMNeT++ non supporta nativamente il linguaggio Python tramite il quale è stato definito l'agente, per integrare simulatore ed agente è stata utilizzata la libreria PyBind\footnote{\href{https://pybind11.readthedocs.io/en/stable/}{https://pybind11.readthedocs.io/en/stable/}} che permette di esporre e fare binding tra simboli definiti in Python e simboli definiti C++. 

\subsubsection{Nodo}
Un nodo corrisponde a un elemento della rete capace di ricevere pacchetti e di inoltrarli 
al resto dei nodi nella rete a seconda delle decisioni intraprese dal suo agente.
È suddiviso nei seguenti componenti (esempio in \autoref{fig:node_layout_3queues}):
\begin{itemize}
    \item Controller: É il componente che implementa la logica applicativa del nodo.
    Tra le sue responsabilità figurano l'interrogazione dell'agente e la conseguente
    attuazione dell'azione suggerita, la gestione delle code e delle power sources,
    il calcolo della reward;
    \item AgentClient: È il componente attraverso il quale il Controller e l'agente
    comunicano;
    \item Queues: una o più code che accodano i pacchetti in arrivo e li consegnano
     al Controller
    dietro una sua richiesta. Ogni coda ha una capacità limitata e una priorità.
\end{itemize}

\subsubsection{Nodo - Controller}
Il Controller è il componente del nodo necessario affinché l'agente possa interagire con
l'ambiente simulato. Esso si preoccupa di:
\begin{itemize}
    \item Campionare lo stato visibile dal nodo;
    \item Calcolare la reward frutto dell'ultima azione effettuata;
    \item Interrogare l'agente per ottenere l'azione da intraprendere fornendogli 
    l'ultimo campione dello stato e la reward calcolata dall'ultima azione;
    \item Interpretare l'azione restituita dall'agente e attuarla;
\end{itemize} 
Queste mansioni sono svolte ciclicamente nell'ordine in cui sono presentate
a intervalli scanditi da un \code{Timeout} dedicato denominato
\code{ask\_action\_timeout}. La durata di tale intervallo
definisce la durata di un timestep dal punto di vista dell'agente.

\subsubsection{Nodo - Controller - Action Execution}
Il controller utilizza l'Agent Client per interfacciarsi con l'agente, fornendogli un
campione dello stato della simulazione e la reward dell'ultima azione intrapresa per
poterlo interrogare sull'azione da intraprendere. Una volta ricevuta la risposta, il
controller interpreta il da farsi (\autoref{fig:controller_doAction_activity_diagram}):
\begin{itemize}
    \item Se l'azione è non fare nulla, il controller procede direttamente al calcolo
    della reward;
    \item Se l'azione è inviare un pacchetto, il controller procede a prelevare dalla
    coda specificata dall'agente un pacchetto inviando una richiesta al componente Queue
    corrispondente. Una volta recuperato il pacchetto, il controller ne simula l'invio
    calcolando il consumo energetico in funzione del power model in uso, della dimensione
    del pacchetto e della banda disponibile. Il controller soddisfa tale
    consumo energetico attingendo alla power source specificata dall'agente, e ne
    registra l'uso per il calcolo della reward.
\end{itemize}
Da notare che se l'agente seleziona la batteria come power source e la carica di cui
dispone non fosse sufficiente a soddisfare la domanda energetica, il controller prosciuga
la batteria e soddisfa la rimanente domanda usando il Power Chord.

\subsubsection{Nodo - Controller - Power Model}
Il Controller si occupa anche di gestire le Power Sources da cui il Nodo può attingere
l'energia necessaria per svolgere le sue operazioni, in particolare l'operazione di 
invio dei pacchetti. Ogni power source implementa un'interfaccia \code{PowerSource}
e quelle disponibili per le operazioni del nodo sono registrate nel vettore
\code{power\_sources}. Le power sources registrate in tale vettore al momento sono: 
\begin{itemize}
    \item Una \code{Battery}: una power source dotata di una capacità, ricaricabile, il
    cui uso costa zero;
    \item Un \code{PowerChord}: una power source che non si scarica mai, ma che ha un
    costo di utilizzo diverso da zero.
\end{itemize}
Tali PowerSources sono istanziate secondo i parametri defini dai rispettivi modelli
indicati nel file di configurazione \code{omnetpp.ini}, che specificano
i valori per la capacità energetica e il costo in reward per unità di energia.

Per simulare la ricarica della batteria è stata definita una nuova \code{PowerSource}
chiamata \code{RandomCharger}, la cui peculiarità è quella di erogare una percentuale
aleatoria dell'energia richiesta secondo una distribuzione specificata alla costruzione.
L'introduzione dell'aleatorietà dovrebbe simulare l'influenza di fattori ambientali
nella ricarica della batteria, ad esempio se tale caricabatteria fosse alimentato da
energia solare e quindi dipendesse dalle condizioni del meteo. Gli istanti di ricarica
della batteria sono scanditi da un timer dedicato denominato
\code{charge\_battery\_timeout}.

I costi energetici delle operazioni del nodo sono anche essi definiti in modelli 
denominati \code{power\_models} e specificati nel file \code{omnetpp.ini}. 

\subsubsection{Nodo - Controller - Reward Computation}

Il calcolo della reward avviene sempre dopo che il controller ha effettuato l'azione
invocando il metodo \code{compute\_reward()}.
La reward è concepita come una riduzione per somma di una serie di termini, ognuno
modellato come un oggetto \code{RewardTerm}. Per ogni termine è possibile specificare
i seguenti attributi:
\begin{itemize}
    \item Un \textit{segnale}, ovvero un'espressione che combina i valori di una o più grandezze osservabili nella simulazione per produrre un valore di reward;
    \item Un \textit{peso}, che moltiplicato per il valore del segnale, ne determina il
    contributo al calcolo della reward finale;
\end{itemize}
I suddetti attributi possono essere specificati nel file di configurazione
\code{omnetpp.ini}.

Per ogni \code{RewardTerm} è possibile specificare un metodo di normalizzazione da
applicare al valore del segnale prima di moltiplicarlo per il peso. Per farlo, è
sufficiente passare alla costruzione del termine un oggetto \code{Normalizer}. In
particolare, per implementare la reward del modello è stato usato un
\code{MinMaxNormalizer} per ogni termine, in maniera tale che la somma pesata di tutti i termini sia compresa tra -1 e 0.

A volte può capitare che l'Agente opti per delle azioni che sono ovviamente inutili,
come ad esempio decidere di inviare un messaggio da una coda vuota. Per scoraggiare
l'agente da tali comportamenti, ogni qual volta un'azione del genere è suggerita
dall'agente il controller invoca \code{illegal\_action\_penalty()} per assegnare
la reward più negativa possibile a tale azione. 

\subsubsection{Nodo - Agent Client}
Ogni nodo dispone di un agente di reinforcement learning che può essere interrogato
dal controller per essere istruito su qual'è la migliore azione da intraprendere in un
determinato momento. L'AgentClient è il componente che si occupa di fare da intermediario
tra il controller del nodo e il suo agente. Il controller periodicamente contatta
 l'AgentClient con una \code{AgentClientRequest}
per poter interrogare l'agente
su quale azione intraprendere. L'AgentClient risponderà con una
\code{AgentClientResponse} contenente una descrizione dell'azione da intraprendere,
che il controller interpreterà ed eseguirà.

L'AgentClient è un componente di interfaccia progettato per poterne sostituire 
l'implemntazione in maniera semplice e veloce, senza dover modificare il controller.
Questo perché nel mondo delle implementazioni disponibili di modelli di machine learning
 esiste un 
ecosistema piuttosto maturo di librerie e framework scritti in linguaggio Python, il
che lo rende una piattaforma appetibile per sviluppare prototipi e avere molta libertà
nel configurarli. Tale ecosistema include anche implementazioni di ambienti in cui
gli agenti delle suddette librerie posso essere addestrati e valutati. Nel caso del
problema, l'ambiente di simulazione è stato implementato usando il framework OMNeT++,
il quale è scritto in linguaggio C++. Nasce dunque l'esigenza di un componente che 
permetta la comunicazione tra il Controller e l'agente di reinforcement learning, i
quali possono essere implementati in tecnologie diverse che non possono comunicare
tra loro senza le dovute accortezze. Ciò permette di sfruttare i vantaggi di tutte le
tecnologie coinvolte con un ragionevole sforzo di integrazione.

Nel caso di studio affrontato è stata implementata una versione dell'AgentClient chiamata
\code{AgentClientPybind}, che sfrutta la libreria PyBind per poter invocare codice
Python da C++. In particolare, le operazioni svolte da tale componente sono:
\begin{itemize}
    \item Accogliere le richieste del controller, le quali contengono un campionamento
    dello stato e la reward dell'azione precedente;
    \item Convertire la richiesta in oggetti Bean compatibili con l'implementazione Python
    dell'agente;
    \item Addestrare l'agente con la reward ricevuta e chiedere l'azione in base 
    allo stato, ottenendo così una \code{ActionBean};
    \item Convertire la \code{ActionBean} in un messaggio inscrivibile 
    in una \code{AgentClientResponse};
    \item Inviare la risposta al controller.
\end{itemize}
Il flusso di esecuzione delle suddette operazioni è illustrato in \autoref{fig:agentc_sequence_diagram}.

Per poter eseguire codice python l'agent client sfrutta un'istanza di
\code{PythonInterpreter}, ovvero una classe singleton con il compito di mantenere un 
riferimento a un interprete Python.
 
\subsubsection{Nodo - Queue}
Ogni nodo dispone di una o più code adibite a ospitare i pacchetti in arrivo.
Il controller del nodo registra ogni coda in un array, e assegna ad ognuna di esse una
priorità pari al suo indice in tale array. Inoltre, ogni coda ha una capacità limitata,
che se raggiunta comporta la perdita di nuovi pacchetti in arrivo. Infine, ogni coda
può gestire più serventi.

L'implementazione della coda è prodotta da una decorazione della classe \code{cQueue}
del framework di OMNeT++. In particolare, sono stati applicati i decoratori:
\begin{itemize}
    \item \code{FixedCapCQueue} che limita la capacità della coda a un valore desiderato.
    La coda dunque accetterà nuovi pacchetti solo se la sua dimensione è minore della
    sua capacità. Se tale condizione non sussiste, l'inserimento di nuovi pacchetti
    provoca il sollevamento di un'eccezione;
    \item \code{PriorityCQueue} che registra la priorità desiderata nella coda;
\end{itemize}
Il modulo della coda mantiene un riferimento a un oggetto \code{cQueue} chiamato
\code{data\_buffer} che viene usato
come coda vera e propria per i pacchetti dati in arrivo.

Una coda può ricevere i seguenti messaggi:
\begin{itemize}
    \item \code{DataMsg}: un pacchetto dati in arrivo nella coda. La coda tenterà di
    inserire tale pacchetto nel suo \code{data\_buffer}. Se il pacchetto è accettato,
    viene accodato e annotato il tempo di arrivo, altrimenti è scartato.
    \item \code{QueueDataRequest}: Un servente vuole prelevare uno o più pacchetti
     dalla coda. La coda tenta di soddisfare la richiesta prelevando un numero di
     pacchetti dal suo buffer che è al più pari al numero di pacchetti richiesti.
     Tali pacchetti sono poi inviati al servente richiedente incartati in una
     \code{QueueDataResponse}.

La coda comunica informazioni relative al suo stato inviando ai suoi serventi un
\code{QueueStateUpdate} contenente le seguenti informazioni:
\begin{itemize}
    \item percentuale di buffer occupato della coda rispetto alla sua capacità;
    \item numero di pacchetti arrivato alla coda (accettati e scartati) dall'ultimo
    aggiornamento sullo stato della coda;
    \item numero di pacchetti scartati dalla coda dall'ultimo aggiornamento sullo stato
    della coda;
\end{itemize}
La coda emette un aggiornamento sul suo stato ogni qual volta riceve un pacchetto, sia
che venga accettato o meno. Inoltre, la coda include un aggiornamento sul suo stato
nella risposta a una richiesta di pacchetti dati da parte di un servente.    
\end{itemize}

Una descrizione grafica dei comportamenti della coda discussi finora è presentata
nei diagrammi in \autoref{fig:queue_handleQueueDataRequest_activity_diagram} e 
\autoref{fig:queue_handleDataMsg_activity_diagram}.

\subsubsection{SrcNode}
E' un componente che ha il compito di generare il traffico da inviare al nodo router, in particolare, ad ogni coda del nodo router è associato un nodo sorgente differente. Il traffico è composto di pacchetti di dimensione casuale che vengono inviati ad intervalli di tempo casuali. La dimensione dei pacchetti è ottenuta campionando da una distribuzione uniforme su un intervallo configurabile che rappresenta la taglia minima e la taglia massima in byte del pacchetto, mentre l'intervallo di tempo tra l'invio di due pacchetti è ottenuto campionando da una distribuzione esponenziale di media configurabile.

\subsubsection{Network}
E' l'ambiente in cui vivono il nodo router e tutte i nodi sorgente, in particolare è qui che viene esplicitata la connessione tra i nodi sorgenti e le code del nodo router. (\autoref{fig:network_layout_3queues}) 

\section{Risultati}
%TODO:

\subsection{Condizioni}

% TODO: una subsection per ogni scenario

\section{Conclusioni}
%TODO:

\subsection{What we have learned?}
%TODO:

\subsection{What we could improve?}
%TODO:

\section{Allegati}

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figs/decisions.drawio.png}
    \caption{Esempio di albero delle decisioni, usando come esempio il combattimento
    nel gioco dei Pokémon. Un esempio di decision path:\\
    \code{\{root: bag, select\_item: potions, potion: iper\}}}
    \label{fig:decisions}
\end{figure*}
\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figs/decisions_getDecisions_activity_diagram.drawio.png}
    \caption{activity diagram per il metodo \code{getDecisions()} di un Consultant}
    \label{fig:decisions_getDecisions_activity_diagram}
\end{figure*}
\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figs/decisions_train_activity_diagram.drawio.png}
    \caption{activity diagram per il metodo \code{train()} di un Consultant}
    \label{fig:decisions_train_activity_diagram}
\end{figure*}
\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figs/queue_handleDataMsg_activity_diagram.drawio.png}
    \caption{activity diagram per la gestione dell'arrivo di un pacchetto nella coda}
    \label{fig:queue_handleDataMsg_activity_diagram}
\end{figure*}
\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figs/queue_handleQueueDataRequest_activity_diagram.drawio.png}
    \caption{activity diagram per la gestione di una richiesta di dati da parte di un servente di pacchetti nella coda}
    \label{fig:queue_handleQueueDataRequest_activity_diagram}
\end{figure*}
\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figs/agentc_sequence_diagram.drawio.png}
    \caption{sequence diagram per la servitura di una richiesta di azione del Controller nei confronti dell'agente.}
    \label{fig:agentc_sequence_diagram}
\end{figure*}
\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figs/node_layout_3queues.png}
    \caption{Nodo con tre code}
    \label{fig:node_layout_3queues}
\end{figure*}
\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figs/network_layout_3queues.png}
    \caption{Rete con tre nodi sorgente e un solo nodo con agente.}
    \label{fig:network_layout_3queues}
\end{figure*}
\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figs/controller_doAction_activity_diagram.drawio.png}
    \caption{\code{do\_action()} Activity diagram. I consumi energetici sono annotati dal controller per poter essere utilizzati dal metodo \code{compute\_reward()} in modo da poter essere combinati con i costi di utilizzo delle rispettive power sources ai fini del calcolo della reward.}
    \label{fig:controller_doAction_activity_diagram}
\end{figure*}


\end{document}